# evaluation.py
# modules/evaluation.py


import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import metrics
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve
import scipy.stats as stats
from sklearn.preprocessing import LabelEncoder

def run_evaluation(model, X_test, y_test):
    st.subheader("üìà √âvaluation du mod√®le")
    
    # Afficher le mod√®le utilis√©
    model_name = st.session_state.get("current_model_name", None)
    best_model_name = st.session_state.get("best_model_name", None)
    
    if model_name:
        # Mod√®le vient de l'affinage
        st.success(f"üéØ **Mod√®le √©valu√©** : {model_name}")
        
        # Afficher les m√©triques si disponibles
        if "evaluation_metrics" in st.session_state and not st.session_state["evaluation_metrics"].empty:
            metrics_df = st.session_state["evaluation_metrics"]
            st.info(f"üìä **Performance** : {metrics_df.to_dict('records')[0]}")
    elif best_model_name:
        # Mod√®le vient de la comparaison
        st.success(f"üèÜ **Meilleur mod√®le de la comparaison** : {best_model_name}")
        
        # R√©cup√©rer le score si disponible
        if "comparison_results" in st.session_state:
            comparison_results = st.session_state["comparison_results"]
            best_row = comparison_results[comparison_results['Mod√®le'] == best_model_name]
            if not best_row.empty:
                score_col = 'Accuracy' if 'Accuracy' in comparison_results.columns else 'R¬≤'
                best_score = best_row[score_col].values[0]
                st.info(f"üìä **Score de la comparaison** : {score_col} = {best_score:.4f}")
    else:
        st.info("‚ÑπÔ∏è Mod√®le entra√Æn√©")
    
    st.markdown("---")
    
    preds = model.predict(X_test)

    # D√©tection automatique du type
    is_classification = y_test.dtype == "object" or y_test.nunique() < 20

    if is_classification:
        st.write("Classification ‚Äî m√©triques :")
        metrics_result = metrics.classification_metrics(y_test, preds)
        metrics_df = pd.DataFrame([metrics_result])
        st.dataframe(metrics_df)
        st.session_state["evaluation_metrics"] = metrics_df
        st.session_state["y_pred"] = preds

        st.write("üìä Choisir graphiques :")
        show_cm = st.checkbox("Matrice de confusion", True)
        show_roc = st.checkbox("Courbe ROC", True)
        show_pr = st.checkbox("Courbe Pr√©cision-Rappel", False)

        if show_cm:
            cm = confusion_matrix(y_test, preds)
            fig, ax = plt.subplots()
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
                        xticklabels=np.unique(y_test),
                        yticklabels=np.unique(y_test))
            ax.set_xlabel("Pr√©dit"); ax.set_ylabel("Vrai")
            st.pyplot(fig)

        # Cas binaire uniquement pour ROC / PR
        if len(np.unique(y_test)) == 2:
            # Encodage des labels texte en 0/1 si n√©cessaire
            le = LabelEncoder()
            y_true_encoded = le.fit_transform(y_test)
            
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X_test)[:, 1]
            else:
                # fallback si pas de predict_proba
                proba = preds if np.issubdtype(preds.dtype, np.number) else y_true_encoded

            if show_roc:
                fpr, tpr, _ = roc_curve(y_true_encoded, proba, pos_label=1)
                roc_auc = auc(fpr, tpr)
                fig, ax = plt.subplots()
                ax.plot(fpr, tpr, label=f"AUC={roc_auc:.2f}")
                ax.plot([0,1],[0,1],"--",color="gray")
                ax.set_xlabel("Faux positifs"); ax.set_ylabel("Vrais positifs"); ax.legend()
                st.pyplot(fig)

            if show_pr:
                precision, recall, _ = precision_recall_curve(y_true_encoded, proba, pos_label=1)
                fig, ax = plt.subplots()
                ax.plot(recall, precision, label="PR Curve")
                ax.set_xlabel("Recall"); ax.set_ylabel("Precision"); ax.set_title("Courbe PR")
                st.pyplot(fig)

    else:
        st.write("R√©gression ‚Äî m√©triques :")
        metrics_result = metrics.regression_metrics(y_test, preds)
        metrics_df = pd.DataFrame([metrics_result])
        st.dataframe(metrics_df)
        st.session_state["evaluation_metrics"] = metrics_df
        st.session_state["y_pred"] = preds
        residuals = y_test - preds
        st.session_state["residuals"] = residuals

        st.write("üìä Choisir graphiques :")
        show_scatter = st.checkbox("Pr√©dit vs R√©el", True)
        show_resid = st.checkbox("R√©sidus vs Pr√©dit", True)
        show_hist = st.checkbox("Histogramme des r√©sidus", True)
        show_qq = st.checkbox("QQ-plot r√©sidus", False)

        if show_scatter:
            fig,ax=plt.subplots(figsize=(6,4))
            ax.scatter(y_test,preds,alpha=0.6)
            ax.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],"r--")
            ax.set_xlabel("R√©el"); ax.set_ylabel("Pr√©dit"); ax.set_title("Pr√©dit vs R√©el")
            st.pyplot(fig)
        if show_resid:
            fig,ax=plt.subplots(figsize=(6,4))
            ax.scatter(preds,residuals,alpha=0.6)
            ax.axhline(0,color="red",linestyle="--")
            ax.set_xlabel("Pr√©dit"); ax.set_ylabel("R√©sidu (y-≈∑)"); ax.set_title("R√©sidus vs Pr√©dit")
            st.pyplot(fig)
        if show_hist:
            fig,ax=plt.subplots(figsize=(6,4))
            sns.histplot(residuals,bins=30,kde=True,ax=ax); ax.set_title("Histogramme des r√©sidus")
            st.pyplot(fig)
        if show_qq:
            fig,ax=plt.subplots(figsize=(6,4))
            stats.probplot(residuals,dist="norm",plot=ax); ax.set_title("QQ-plot des r√©sidus")
            st.pyplot(fig)