# evaluation.py
# modules/evaluation.py


import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import metrics
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve
import scipy.stats as stats
from sklearn.preprocessing import LabelEncoder
from advanced_evaluation import run_advanced_evaluation

def run_evaluation(X_test, y_test):
    """
    Interface d'√©valuation du mod√®le avec s√©lection du mod√®le √† √©valuer
    """
    # R√©cup√©rer les mod√®les disponibles
    refined_model = st.session_state.get("model", None)
    best_model = st.session_state.get("best_model", None)
    refined_model_name = st.session_state.get("current_model_name", None)
    best_model_name = st.session_state.get("best_model_name", None)
    best_model_score = st.session_state.get("best_model_score", None)
    
    # D√©terminer quel mod√®le √©valuer
    model_to_evaluate = None
    model_display_name = None
    
    # Debug : Afficher ce qui est disponible en session state
    if st.checkbox("üîç Debug - Voir les mod√®les disponibles"):
        st.write("**Session state des mod√®les :**")
        st.write(f"- refined_model (model): {'‚úÖ Disponible' if refined_model else '‚ùå Non disponible'}")
        st.write(f"- best_model: {'‚úÖ Disponible' if best_model else '‚ùå Non disponible'}")
        st.write(f"- refined_model_name: {refined_model_name}")
        st.write(f"- best_model_name: {best_model_name}")
        st.write(f"- best_model_score: {best_model_score}")
    
    # Logique am√©lior√©e de s√©lection du mod√®le
    if best_model is not None:
        # Si on a un best_model (venant de la comparaison), c'est le mod√®le principal
        if refined_model is not None and refined_model_name != best_model_name:
            # Cas : l'utilisateur a fait une comparaison puis un affinage
            st.info("üéØ Vous avez affin√© un mod√®le apr√®s la comparaison. Quel mod√®le souhaitez-vous √©valuer ?")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button(f"üîß **Mod√®le affin√©**\n{refined_model_name}", use_container_width=True, type="primary"):
                    st.session_state["selected_eval_model"] = "refined"
            with col2:
                score_text = f" (Score: {best_model_score:.4f})" if best_model_score else ""
                if st.button(f"üèÜ **Meilleur mod√®le de la comparaison**\n{best_model_name}{score_text}", use_container_width=True):
                    st.session_state["selected_eval_model"] = "best"
            
            # D√©terminer le mod√®le s√©lectionn√©
            selected = st.session_state.get("selected_eval_model", "best")  # Par d√©faut: best_model
            if selected == "refined":
                model_to_evaluate = refined_model
                model_display_name = refined_model_name
            else:
                model_to_evaluate = best_model
                model_display_name = best_model_name
        else:
            # Cas : seulement le best_model disponible (apr√®s comparaison)
            model_to_evaluate = best_model
            model_display_name = best_model_name or "Meilleur mod√®le"
            st.success(f"üìä **Mod√®le √©valu√©** : {model_display_name}")
    
    elif refined_model is not None:
        # Cas : seulement un mod√®le affin√© (sans comparaison pr√©alable)
        model_to_evaluate = refined_model
        model_display_name = refined_model_name or "Mod√®le affin√©"
        st.success(f"üìä **Mod√®le √©valu√©** : {model_display_name}")
    
    else:
        st.error("‚ùå Aucun mod√®le disponible pour l'√©valuation.")
        st.info("üí° **Solution** : Entra√Ænez d'abord un mod√®le via la comparaison ou l'affinage.")
        return
    
    preds = model_to_evaluate.predict(X_test)

    # D√©tection automatique du type
    is_classification = y_test.dtype == "object" or y_test.nunique() < 20

    if is_classification:
        st.write("Classification ‚Äî m√©triques :")
        metrics_result = metrics.classification_metrics(y_test, preds)
        metrics_df = pd.DataFrame([metrics_result])
        st.dataframe(metrics_df)
        st.session_state["evaluation_metrics"] = metrics_df
        st.session_state["y_pred"] = preds

        st.write("üìä Choisir graphiques :")
        show_cm = st.checkbox("Matrice de confusion", True)
        show_roc = st.checkbox("Courbe ROC", True)
        show_pr = st.checkbox("Courbe Pr√©cision-Rappel", False)

        if show_cm:
            cm = confusion_matrix(y_test, preds)
            fig, ax = plt.subplots()
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
                        xticklabels=np.unique(y_test),
                        yticklabels=np.unique(y_test))
            ax.set_xlabel("Pr√©dit"); ax.set_ylabel("Vrai")
            st.pyplot(fig)

        # Cas binaire uniquement pour ROC / PR
        if len(np.unique(y_test)) == 2:
            # Encodage des labels texte en 0/1 si n√©cessaire
            le = LabelEncoder()
            y_true_encoded = le.fit_transform(y_test)
            
            if hasattr(model_to_evaluate, "predict_proba"):
                proba = model_to_evaluate.predict_proba(X_test)[:, 1]
            else:
                # fallback si pas de predict_proba
                proba = preds if np.issubdtype(preds.dtype, np.number) else y_true_encoded

            if show_roc:
                fpr, tpr, _ = roc_curve(y_true_encoded, proba, pos_label=1)
                roc_auc = auc(fpr, tpr)
                fig, ax = plt.subplots()
                ax.plot(fpr, tpr, label=f"AUC={roc_auc:.2f}")
                ax.plot([0,1],[0,1],"--",color="gray")
                ax.set_xlabel("Faux positifs"); ax.set_ylabel("Vrais positifs"); ax.legend()
                st.pyplot(fig)

            if show_pr:
                precision, recall, _ = precision_recall_curve(y_true_encoded, proba, pos_label=1)
                fig, ax = plt.subplots()
                ax.plot(recall, precision, label="PR Curve")
                ax.set_xlabel("Recall"); ax.set_ylabel("Precision"); ax.set_title("Courbe PR")
                st.pyplot(fig)
    
    else:
        st.write("R√©gression ‚Äî m√©triques :")
        metrics_result = metrics.regression_metrics(y_test, preds)
        metrics_df = pd.DataFrame([metrics_result])
        st.dataframe(metrics_df)
        st.session_state["evaluation_metrics"] = metrics_df
        st.session_state["y_pred"] = preds
        residuals = y_test - preds
        st.session_state["residuals"] = residuals

        st.write("üìä Choisir graphiques :")
        show_scatter = st.checkbox("Pr√©dit vs R√©el", True)
        show_resid = st.checkbox("R√©sidus vs Pr√©dit", True)
        show_hist = st.checkbox("Histogramme des r√©sidus", True)
        show_qq = st.checkbox("QQ-plot r√©sidus", False)

        if show_scatter:
            fig,ax=plt.subplots(figsize=(6,4))
            ax.scatter(y_test,preds,alpha=0.6)
            ax.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],"r--")
            ax.set_xlabel("R√©el"); ax.set_ylabel("Pr√©dit"); ax.set_title("Pr√©dit vs R√©el")
            st.pyplot(fig)
        if show_resid:
            fig,ax=plt.subplots(figsize=(6,4))
            ax.scatter(preds,residuals,alpha=0.6)
            ax.axhline(0,color="red",linestyle="--")
            ax.set_xlabel("Pr√©dit"); ax.set_ylabel("R√©sidu (y-≈∑)"); ax.set_title("R√©sidus vs Pr√©dit")
            st.pyplot(fig)
        if show_hist:
            fig,ax=plt.subplots(figsize=(6,4))
            sns.histplot(residuals,bins=30,kde=True,ax=ax); ax.set_title("Histogramme des r√©sidus")
            st.pyplot(fig)
        if show_qq:
            fig,ax=plt.subplots(figsize=(6,4))
            stats.probplot(residuals,dist="norm",plot=ax); ax.set_title("QQ-plot des r√©sidus")
            st.pyplot(fig)
    
    # Section d'√©valuation avanc√©e
    st.markdown("---")
    st.markdown("## üî¨ √âvaluation Avanc√©e")
    st.info("üí° **Analyses sophistiqu√©es** pour comprendre en profondeur votre mod√®le")
    
    # Bouton pour lancer l'√©valuation avanc√©e
    if st.button("üöÄ Lancer l'√âvaluation Avanc√©e", type="primary"):
        task_type = "classification" if is_classification else "regression"
        run_advanced_evaluation(model_to_evaluate, X_test, y_test, task_type)