# modules/model_comparison.py
"""
Module de comparaison de plusieurs mod√®les ML
Permet d'entra√Æner et comparer simultan√©ment plusieurs algorithmes
"""

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.ensemble import (
    RandomForestClassifier, RandomForestRegressor,
    GradientBoostingClassifier, GradientBoostingRegressor,
    AdaBoostClassifier, AdaBoostRegressor,
    ExtraTreesClassifier, ExtraTreesRegressor
)
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.svm import SVC, SVR
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
import joblib
import time
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
import seaborn as sns
import helpers
import metrics
import model_utils  # Import du nouveau module d'utilitaires


class ModelComparator:
    """Classe pour comparer plusieurs mod√®les ML"""
    
    def __init__(self, task: str = "classification"):
        """
        Initialise le comparateur de mod√®les
        
        Args:
            task: Type de t√¢che ("classification" ou "regression")
        """
        self.task = task
        self.results = []
        self.models = {}
        self.best_model = None
        self.best_score = -np.inf if task == "classification" else np.inf
        
    def get_available_models(self) -> Dict[str, Any]:
        """Retourne les mod√®les disponibles selon la t√¢che"""
        if self.task == "classification":
            return {
                "Random Forest": RandomForestClassifier(random_state=42),
                "Gradient Boosting": GradientBoostingClassifier(random_state=42),
                "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
                "Decision Tree": DecisionTreeClassifier(random_state=42),
                "AdaBoost": AdaBoostClassifier(random_state=42),
                "Extra Trees": ExtraTreesClassifier(random_state=42),
                "K-Nearest Neighbors": KNeighborsClassifier(),
                "SVM": SVC(probability=True, random_state=42),
                "Naive Bayes": GaussianNB()
            }
        else:  # regression
            return {
                "Random Forest": RandomForestRegressor(random_state=42),
                "Gradient Boosting": GradientBoostingRegressor(random_state=42),
                "Linear Regression": LinearRegression(),
                "Ridge": Ridge(random_state=42),
                "Lasso": Lasso(random_state=42),
                "Decision Tree": DecisionTreeRegressor(random_state=42),
                "AdaBoost": AdaBoostRegressor(random_state=42),
                "Extra Trees": ExtraTreesRegressor(random_state=42),
                "K-Nearest Neighbors": KNeighborsRegressor(),
                "SVR": SVR()
            }
    
    def build_preprocessor(self, X: pd.DataFrame, do_scale: bool = True) -> ColumnTransformer:
        """Construit le preprocesseur pour les donn√©es"""
        # Utilisation de la fonction commune de model_utils
        return model_utils.build_preprocessor(X, do_scale)
    
    def train_and_evaluate(
        self,
        model_name: str,
        model: Any,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        preprocessor: ColumnTransformer,
        use_cv: bool = False,
        cv_folds: int = 5
    ) -> Dict[str, Any]:
        """
        Entra√Æne et √©value un mod√®le
        
        Returns:
            Dictionnaire avec les r√©sultats
        """
        start_time = time.time()
        
        # Cr√©er le pipeline
        pipe = Pipeline([
            ("preprocessor", preprocessor),
            ("model", model)
        ])
        
        # Entra√Ænement
        try:
            pipe.fit(X_train, y_train)
            training_time = time.time() - start_time
            
            # Pr√©dictions
            y_pred = pipe.predict(X_test)
            
            # M√©triques
            if self.task == "classification":
                metrics_result = metrics.classification_metrics(y_test, y_pred)
                primary_metric = metrics_result.get("accuracy", 0)
            else:
                metrics_result = metrics.regression_metrics(y_test, y_pred)
                primary_metric = metrics_result.get("r2", 0)
            
            # Cross-validation si demand√©
            cv_score = None
            if use_cv:
                try:
                    cv_scores = cross_val_score(
                        pipe, X_train, y_train,
                        cv=cv_folds,
                        scoring='accuracy' if self.task == "classification" else 'r2'
                    )
                    cv_score = cv_scores.mean()
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è CV √©chou√© pour {model_name}: {str(e)}")
                    cv_score = None
            
            result = {
                "model_name": model_name,
                "pipeline": pipe,
                "metrics": metrics_result,
                "primary_metric": primary_metric,
                "cv_score": cv_score,
                "training_time": training_time,
                "predictions": y_pred,
                "status": "success"
            }
            
            # Mise √† jour du meilleur mod√®le
            if self.task == "classification":
                if primary_metric > self.best_score:
                    self.best_score = primary_metric
                    self.best_model = model_name
            else:
                if primary_metric > self.best_score:
                    self.best_score = primary_metric
                    self.best_model = model_name
            
            return result
            
        except Exception as e:
            return {
                "model_name": model_name,
                "pipeline": None,
                "metrics": {},
                "primary_metric": 0,
                "cv_score": None,
                "training_time": time.time() - start_time,
                "predictions": None,
                "status": "failed",
                "error": str(e)
            }
    
    def compare_models(
        self,
        selected_models: List[str],
        X_train: pd.DataFrame,
        X_test: pd.DataFrame,
        y_train: pd.Series,
        y_test: pd.Series,
        do_scale: bool = True,
        use_cv: bool = False,
        cv_folds: int = 5
    ) -> pd.DataFrame:
        """
        Compare plusieurs mod√®les
        
        Returns:
            DataFrame avec les r√©sultats de comparaison
        """
        available_models = self.get_available_models()
        preprocessor = self.build_preprocessor(X_train, do_scale)
        
        self.results = []
        self.models = {}
        
        # Barre de progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, model_name in enumerate(selected_models):
            status_text.text(f"Entra√Ænement de {model_name}... ({idx+1}/{len(selected_models)})")
            
            model = available_models[model_name]
            result = self.train_and_evaluate(
                model_name, model,
                X_train, X_test, y_train, y_test,
                preprocessor, use_cv, cv_folds
            )
            
            self.results.append(result)
            if result["status"] == "success":
                self.models[model_name] = result["pipeline"]
            
            progress_bar.progress((idx + 1) / len(selected_models))
        
        status_text.text("‚úÖ Entra√Ænement termin√©!")
        progress_bar.empty()
        
        # Cr√©er le DataFrame de r√©sultats
        return self._create_results_dataframe()
    
    def _create_results_dataframe(self) -> pd.DataFrame:
        """Cr√©e un DataFrame avec les r√©sultats de comparaison"""
        rows = []
        
        for result in self.results:
            if result["status"] == "success":
                row = {
                    "Mod√®le": result["model_name"],
                    "Temps (s)": round(result["training_time"], 2),
                    "Statut": "‚úÖ Succ√®s"
                }
                
                # Ajouter les m√©triques
                for metric_name, metric_value in result["metrics"].items():
                    try:
                        row[metric_name.upper()] = round(float(metric_value), 4)
                    except:
                        row[metric_name.upper()] = metric_value
                
                # Ajouter CV score si disponible
                if result["cv_score"] is not None:
                    row["CV Score"] = round(result["cv_score"], 4)
                
                rows.append(row)
            else:
                rows.append({
                    "Mod√®le": result["model_name"],
                    "Temps (s)": round(result["training_time"], 2),
                    "Statut": f"‚ùå √âchec: {result.get('error', 'Unknown')}"
                })
        
        df = pd.DataFrame(rows)
        
        # Trier par m√©trique principale
        if self.task == "classification" and "ACCURACY" in df.columns:
            df = df.sort_values("ACCURACY", ascending=False)
        elif self.task == "regression" and "R2" in df.columns:
            df = df.sort_values("R2", ascending=False)
        
        return df.reset_index(drop=True)
    
    def plot_comparison(self, results_df: pd.DataFrame) -> plt.Figure:
        """Cr√©e un graphique de comparaison des mod√®les"""
        # Filtrer les mod√®les r√©ussis
        success_df = results_df[results_df["Statut"] == "‚úÖ Succ√®s"].copy()
        
        if len(success_df) == 0:
            st.warning("Aucun mod√®le n'a r√©ussi l'entra√Ænement")
            return None
        
        # D√©terminer la m√©trique principale
        if self.task == "classification":
            metric_col = "ACCURACY" if "ACCURACY" in success_df.columns else success_df.columns[2]
        else:
            metric_col = "R2" if "R2" in success_df.columns else success_df.columns[2]
        
        # Cr√©er le graphique
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # Graphique 1: M√©trique principale
        colors = ['#2ecc71' if model == self.best_model else '#3498db' 
                  for model in success_df["Mod√®le"]]
        
        ax1.barh(success_df["Mod√®le"], success_df[metric_col], color=colors)
        ax1.set_xlabel(metric_col)
        ax1.set_title(f"Comparaison - {metric_col}")
        ax1.grid(axis='x', alpha=0.3)
        
        # Graphique 2: Temps d'entra√Ænement
        ax2.barh(success_df["Mod√®le"], success_df["Temps (s)"], color='#e74c3c')
        ax2.set_xlabel("Temps (secondes)")
        ax2.set_title("Temps d'Entra√Ænement")
        ax2.grid(axis='x', alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def plot_metrics_heatmap(self, results_df: pd.DataFrame) -> plt.Figure:
        """Cr√©e une heatmap des m√©triques pour tous les mod√®les"""
        success_df = results_df[results_df["Statut"] == "‚úÖ Succ√®s"].copy()
        
        if len(success_df) == 0:
            return None
        
        # S√©lectionner uniquement les colonnes num√©riques (m√©triques)
        metric_cols = success_df.select_dtypes(include=[np.number]).columns.tolist()
        metric_cols = [col for col in metric_cols if col != "Temps (s)"]
        
        if len(metric_cols) == 0:
            return None
        
        # Cr√©er la heatmap
        fig, ax = plt.subplots(figsize=(10, len(success_df) * 0.5 + 2))
        
        heatmap_data = success_df[["Mod√®le"] + metric_cols].set_index("Mod√®le")
        
        sns.heatmap(
            heatmap_data,
            annot=True,
            fmt=".3f",
            cmap="RdYlGn",
            center=0.5,
            ax=ax,
            cbar_kws={'label': 'Score'}
        )
        
        ax.set_title("Heatmap des M√©triques par Mod√®le")
        plt.tight_layout()
        return fig
    
    def save_best_model(self, target: str, output_dir: str = "outputs/models") -> str:
        """Sauvegarde le meilleur mod√®le"""
        if self.best_model is None or self.best_model not in self.models:
            raise ValueError("Aucun meilleur mod√®le trouv√©")
        
        helpers.ensure_dir(output_dir)
        model_path = f"{output_dir}/best_model_{target}.pkl"
        joblib.dump(self.models[self.best_model], model_path)
        
        return model_path
    
    def get_model_details(self, model_name: str) -> Dict[str, Any]:
        """Retourne les d√©tails d'un mod√®le sp√©cifique"""
        for result in self.results:
            if result["model_name"] == model_name:
                return result
        return None


def run_model_comparison(df: pd.DataFrame) -> dict:
    """
    Interface Streamlit pour la comparaison de mod√®les
    
    Args:
        df: DataFrame avec les donn√©es
        
    Returns:
        Dictionnaire avec les r√©sultats
    """
    st.subheader("üî¨ Comparaison de Mod√®les ML")
    
    # S√©lection de la variable cible
    cols = df.columns.tolist()
    target = st.selectbox("Choisir la variable cible", [""] + cols, key="comparison_target")
    
    if not target:
        st.info("S√©lectionnez une variable cible pour commencer la comparaison.")
        st.stop()
    
    X = df.drop(columns=[target])
    y = df[target]
    
    # Validation de la cible avec model_utils
    y, valid_idx = model_utils.validate_and_clean_target(y, target)
    if not valid_idx.all():
        X = X[valid_idx].reset_index(drop=True)
    
    # D√©tection automatique de la t√¢che avec model_utils
    task = model_utils.detect_task_type(y)
    st.info(f"üìä T√¢che d√©tect√©e : **{task.upper()}**")
    
    # Afficher les statistiques avec model_utils
    model_utils.display_target_stats(y, task)
    
    # Configuration
    st.markdown("### ‚öôÔ∏è Configuration")
    
    col1, col2 = st.columns(2)
    with col1:
        test_size = st.slider("Taille test (%)", 5, 50, 20, key="comp_test_size") / 100.0
        do_scale = st.checkbox("Standardiser les variables num√©riques", value=True, key="comp_scale")
    
    with col2:
        random_state = int(st.number_input("Seed al√©atoire", value=42, key="comp_seed"))
        use_cv = st.checkbox("Utiliser la validation crois√©e", value=False, key="comp_cv")
        if use_cv:
            cv_folds = int(st.number_input("Nombre de folds", 3, 10, 5, key="comp_cv_folds"))
        else:
            cv_folds = 5
    
    # S√©lection des mod√®les
    st.markdown("### üéØ S√©lection des Mod√®les")
    
    comparator = ModelComparator(task=task)
    available_models = list(comparator.get_available_models().keys())
    
    # Initialiser selected_models dans session_state si n√©cessaire
    if "selected_models" not in st.session_state:
        st.session_state.selected_models = ["Random Forest", "Gradient Boosting"]
    
    # Options de s√©lection rapide
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("‚úÖ Tout s√©lectionner"):
            st.session_state.selected_models = available_models
            st.rerun()
    with col2:
        if st.button("üöÄ Mod√®les rapides"):
            # Utilisation de la fonction commune
            st.session_state.selected_models = model_utils.get_fast_models(task)
            st.rerun()
    with col3:
        if st.button("‚ùå Tout d√©s√©lectionner"):
            st.session_state.selected_models = []
            st.rerun()
    
    # Multiselect pour les mod√®les
    selected_models = st.multiselect(
        "Choisir les mod√®les √† comparer",
        available_models,
        default=st.session_state.selected_models,
        key="model_multiselect"
    )
    
    # Mettre √† jour session_state avec la s√©lection actuelle
    st.session_state.selected_models = selected_models
    
    if len(selected_models) == 0:
        st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins un mod√®le")
        st.stop()
    
    st.info(f"üìä {len(selected_models)} mod√®le(s) s√©lectionn√©(s)")
    
    # Bouton de lancement
    if st.button("üöÄ Lancer la Comparaison", type="primary"):
        with st.spinner("Entra√Ænement en cours..."):
            # Split des donn√©es
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            
            # Comparaison
            results_df = comparator.compare_models(
                selected_models,
                X_train, X_test, y_train, y_test,
                do_scale=do_scale,
                use_cv=use_cv,
                cv_folds=cv_folds
            )
            
            # Affichage des r√©sultats
            st.markdown("---")
            st.markdown("## üìä R√©sultats de la Comparaison")
            
            # Tableau des r√©sultats
            st.dataframe(
                results_df.style.highlight_max(
                    subset=[col for col in results_df.columns if col not in ["Mod√®le", "Temps (s)", "Statut"]],
                    color='lightgreen'
                ),
                use_container_width=True
            )
            
            # Meilleur mod√®le
            if comparator.best_model:
                st.success(f"üèÜ **Meilleur mod√®le** : {comparator.best_model} (Score: {comparator.best_score:.4f})")
            
            # Graphiques de comparaison
            st.markdown("### üìà Visualisations")
            
            tab1, tab2 = st.tabs(["üìä Comparaison", "üî• Heatmap"])
            
            with tab1:
                fig = comparator.plot_comparison(results_df)
                if fig:
                    st.pyplot(fig)
            
            with tab2:
                fig = comparator.plot_metrics_heatmap(results_df)
                if fig:
                    st.pyplot(fig)
                else:
                    st.info("Pas assez de m√©triques pour cr√©er une heatmap")
            
            # Export des r√©sultats
            st.markdown("### üíæ Export")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Export CSV des r√©sultats
                csv = results_df.to_csv(index=False)
                st.download_button(
                    "üì• T√©l√©charger les r√©sultats (CSV)",
                    csv,
                    f"comparison_results_{target}.csv",
                    "text/csv"
                )
            
            with col2:
                # Sauvegarder le meilleur mod√®le
                if st.button("üíæ Sauvegarder le meilleur mod√®le"):
                    try:
                        model_path = comparator.save_best_model(target)
                        st.success(f"‚úÖ Meilleur mod√®le sauvegard√© : {model_path}")
                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de la sauvegarde : {str(e)}")
            
            # Stocker dans session_state de mani√®re coh√©rente avec model_utils
            best_model_pipeline = comparator.models.get(comparator.best_model)
            
            # Stocker le mod√®le avec la fonction commune
            model_utils.store_model_in_session(
                model=best_model_pipeline,
                X_train=X_train,
                X_test=X_test,
                y_train=y_train,
                y_test=y_test,
                task=task,
                model_name=comparator.best_model
            )
            
            # Stocker les r√©sultats de comparaison
            st.session_state.update({
                "comparison_results": results_df,
                "comparator": comparator,
                "best_model": best_model_pipeline
            })
            
            return {
                "results": results_df,
                "best_model": comparator.best_model,
                "comparator": comparator
            }
    
    st.stop()
