# modules/modeling.py
import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
import joblib
from typing import Tuple, Any
import helpers
import metrics
from math import isfinite

def _format_metrics(d: dict, decimals=3):
    """Arrondit les valeurs num√©riques du dict pour l'affichage."""
    out = {}
    for k, v in d.items():
        try:
            fv = float(v)
            if not isfinite(fv):
                out[k] = v
            else:
                out[k] = round(fv, decimals)
        except Exception:
            out[k] = v
    return out

def run_modeling(df: pd.DataFrame) -> dict:
    st.subheader("‚ö° Mod√©lisation interactive")

    cols = df.columns.tolist()
    target = st.selectbox("Choisir la variable cible", [""] + cols)
    if not target:
        st.info("S√©lectionne une variable cible pour lancer l'entra√Ænement.")
        st.stop()

    X = df.drop(columns=[target])
    y = df[target]
    
    # Validation et nettoyage de la variable cible
    st.markdown("### üîç Validation des Donn√©es")
    
    # V√©rifier les valeurs manquantes dans y
    y_missing = y.isna().sum()
    if y_missing > 0:
        st.warning(f"‚ö†Ô∏è Variable cible contient {y_missing} valeurs manquantes ({y_missing/len(y)*100:.1f}%)")
        
        action = st.radio(
            "Comment traiter les valeurs manquantes dans la cible ?",
            ["Supprimer les lignes", "Imputer (moyenne/mode)", "Annuler"],
            key="missing_target_action"
        )
        
        if action == "Annuler":
            st.info("Veuillez nettoyer vos donn√©es avant la mod√©lisation")
            st.stop()
        elif action == "Supprimer les lignes":
            valid_idx = y.notna()
            X = X[valid_idx].reset_index(drop=True)
            y = y[valid_idx].reset_index(drop=True)
            st.success(f"‚úÖ {y_missing} lignes supprim√©es. Nouvelles dimensions : {len(y)} lignes")
        else:  # Imputer
            if y.dtype in ['object', 'category']:
                mode_val = y.mode()[0] if not y.mode().empty else y.dropna().iloc[0]
                y = y.fillna(mode_val)
                st.success(f"‚úÖ Valeurs manquantes imput√©es avec le mode : {mode_val}")
            else:
                mean_val = y.mean()
                y = y.fillna(mean_val)
                st.success(f"‚úÖ Valeurs manquantes imput√©es avec la moyenne : {mean_val:.2f}")
    
    # V√©rifier les valeurs infinies dans y (pour r√©gression)
    if y.dtype in ['int64', 'float64']:
        y_inf = (~y.isna() & ((y == float('inf')) | (y == float('-inf')))).sum()
        if y_inf > 0:
            st.warning(f"‚ö†Ô∏è Variable cible contient {y_inf} valeurs infinies")
            y = y.replace([float('inf'), float('-inf')], pd.NA)
            y = y.fillna(y.median())
            st.success(f"‚úÖ Valeurs infinies remplac√©es par la m√©diane")
    
    # Afficher statistiques de la cible
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Lignes valides", len(y))
    with col2:
        st.metric("Valeurs uniques", y.nunique())
    with col3:
        if y.dtype in ['int64', 'float64']:
            st.metric("Moyenne", f"{y.mean():.2f}")
        else:
            st.metric("Mode", y.mode()[0] if not y.mode().empty else "N/A")
    
    st.markdown("---")

    task = st.selectbox("Type de t√¢che", ["auto", "classification", "regression"], index=0)
    if task == "auto":
        if y.dtype == "O" or (y.nunique() <= 20 and y.nunique()/len(y) < 0.1):
            task = "classification"
        else:
            task = "regression"
    st.write("üëâ T√¢che d√©tect√©e :", task)

    test_size = st.slider("Taille test (%)", 5, 50, 20) / 100.0
    random_state = int(st.number_input("Seed al√©atoire", value=42))

    model_choice = st.selectbox("Choisir un mod√®le", ["auto", "random_forest", "gradient_boosting", "linear/logistic"])
    do_scale = st.checkbox("‚öôÔ∏è Standardiser les num√©riques", value=True)

    # Hyperparam√®tres expos√©s
    if model_choice in ["random_forest", "auto"]:
        rf_n_estimators = int(st.number_input("RF - n_estimators", 10, 1000, 100))
        rf_max_depth = int(st.number_input("RF - max_depth (0=>None)", 0, 50, 0))
    else:
        rf_n_estimators = 100; rf_max_depth = 0

    if model_choice in ["gradient_boosting", "auto"]:
        gb_n_estimators = int(st.number_input("GB - n_estimators", 10, 1000, 100))
        gb_max_depth = int(st.number_input("GB - max_depth", 1, 20, 3))
        gb_lr = float(st.number_input("GB - learning_rate", 0.01, 1.0, 0.1))
    else:
        gb_n_estimators = 100; gb_max_depth = 3; gb_lr = 0.1

    if st.button("üöÄ Lancer l'entra√Ænement"):
        # Pr√©processing pipeline (construit sans boucle co√ªteuse)
        num_cols = X.select_dtypes(include="number").columns.tolist()
        cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
        if cat_cols:
            X[cat_cols] = X[cat_cols].astype(str)

        num_steps = []
        if num_cols:
            num_steps = [("imputer", SimpleImputer(strategy="median"))]
            if do_scale:
                num_steps.append(("scaler", StandardScaler()))

        cat_steps = []
        if cat_cols:
            cat_steps = [("imputer", SimpleImputer(strategy="most_frequent")), ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))]

        transformers = []
        if num_cols:
            transformers.append(("num", Pipeline(num_steps), num_cols))
        if cat_cols:
            transformers.append(("cat", Pipeline(cat_steps), cat_cols))

        preprocessor = ColumnTransformer(transformers=transformers, remainder="drop", verbose_feature_names_out=False)

        # Choix du mod√®le (sans boucle)
        if model_choice == "auto":
            if task == "classification":
                model = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=None if rf_max_depth==0 else rf_max_depth, random_state=random_state)
            else:
                model = RandomForestRegressor(n_estimators=rf_n_estimators, max_depth=None if rf_max_depth==0 else rf_max_depth, random_state=random_state)
        elif model_choice == "random_forest":
            model = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=None if rf_max_depth==0 else rf_max_depth, random_state=random_state) if task=="classification" else RandomForestRegressor(n_estimators=rf_n_estimators, max_depth=None if rf_max_depth==0 else rf_max_depth, random_state=random_state)
        elif model_choice == "gradient_boosting":
            model = GradientBoostingClassifier(n_estimators=gb_n_estimators, max_depth=gb_max_depth, learning_rate=gb_lr, random_state=random_state) if task=="classification" else GradientBoostingRegressor(n_estimators=gb_n_estimators, max_depth=gb_max_depth, learning_rate=gb_lr, random_state=random_state)
        else:
            model = LogisticRegression(max_iter=1000) if task=="classification" else LinearRegression()

        pipe = Pipeline([("preprocessor", preprocessor), ("model", model)])

        # Split & train
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_test)

        # √âvaluation (metrics utilitaires)
        if task == "classification":
            metrics_result = metrics.classification_metrics(y_test, preds)
        else:
            metrics_result = metrics.regression_metrics(y_test, preds)

        metrics_display = _format_metrics(metrics_result, decimals=4)
        st.write("üìä **Metrics (test)** :")
        st.json(metrics_display)

        # Sauvegarde mod√®le et datasets
        helpers.ensure_dir("outputs/models")
        model_path = f"outputs/models/model_{target}.pkl"
        joblib.dump(pipe, model_path)
        st.success(f"‚úÖ Mod√®le entra√Æn√© et sauvegard√© : {model_path}")

        helpers.ensure_dir("outputs/data")
        X_train.assign(**{target: y_train}).to_csv(f"outputs/data/train_{target}.csv", index=False)
        X_test.assign(**{target: y_test}).to_csv(f"outputs/data/test_{target}.csv", index=False)

        # Stocker dans session_state pour reporting/evaluation
        st.session_state.update({
            "model": pipe,
            "X_train": X_train,
            "X_test": X_test,
            "y_train": y_train,
            "y_test": y_test,
            "task": task,
            "y_pred": preds,
            "evaluation_metrics": pd.DataFrame([metrics_display])
        })

        # Feature importance si disponible (essayer d'extraire proprement)
        try:
            m = pipe.named_steps["model"]
            if hasattr(m, "feature_importances_") and len(num_cols) > 0:
                # get feature names from preprocessor
                try:
                    feature_names = pipe.named_steps["preprocessor"].get_feature_names_out()
                except:
                    feature_names = num_cols + cat_cols
                fi = pd.Series(m.feature_importances_, index=feature_names).sort_values(ascending=False)
                st.session_state["feature_importance"] = fi
        except Exception:
            pass

        return {
            "pipeline": pipe,
            "X_train": X_train,
            "X_test": X_test,
            "y_train": y_train,
            "y_test": y_test,
            "task": task
        }

    st.stop()