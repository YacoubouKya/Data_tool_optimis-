# modules/modeling.py
import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.ensemble import (
    RandomForestClassifier, RandomForestRegressor,
    GradientBoostingClassifier, GradientBoostingRegressor,
    AdaBoostClassifier, AdaBoostRegressor,
    ExtraTreesClassifier, ExtraTreesRegressor
)
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.svm import SVC, SVR
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from category_encoders import TargetEncoder
import joblib
from typing import Tuple, Any, Dict
import helpers
import metrics
import model_utils
from math import isfinite

def _format_metrics(d: dict, decimals=3):
    """Arrondit les valeurs num√©riques du dict pour l'affichage."""
    out = {}
    for k, v in d.items():
        try:
            fv = float(v)
            if not isfinite(fv):
                out[k] = v
            else:
                out[k] = round(fv, decimals)
        except Exception:
            out[k] = v
    return out

def _validate_data_for_modeling(X: pd.DataFrame, y: pd.Series) -> bool:
    """
    Valide que les donn√©es sont pr√™tes pour la mod√©lisation
    Retourne True si les donn√©es sont valides, False sinon
    """
    st.markdown("#### üîç Validation pr√©-mod√©lisation")
    
    validation_passed = True
    warnings = []
    errors = []
    
    # 1. V√©rifier que X n'est pas vide
    if X.shape[0] == 0:
        errors.append("‚ùå Le DataFrame X est vide (0 lignes)")
        validation_passed = False
    
    if X.shape[1] == 0:
        errors.append("‚ùå Le DataFrame X n'a aucune colonne (features)")
        validation_passed = False
    
    # 2. V√©rifier que y n'est pas vide
    if len(y) == 0:
        errors.append("‚ùå La variable cible y est vide")
        validation_passed = False
    
    # 3. V√©rifier que X et y ont la m√™me longueur
    if len(X) != len(y):
        errors.append(f"‚ùå Incompatibilit√© de taille : X a {len(X)} lignes mais y a {len(y)} valeurs")
        validation_passed = False
    
    # 4. V√©rifier les NaN dans X
    nan_cols = X.columns[X.isna().any()].tolist()
    if nan_cols:
        nan_count = len(nan_cols)
        if nan_count <= 5:
            warnings.append(f"‚ö†Ô∏è {nan_count} colonne(s) avec valeurs manquantes : {', '.join(nan_cols)}")
        else:
            warnings.append(f"‚ö†Ô∏è {nan_count} colonnes avec valeurs manquantes (dont {', '.join(nan_cols[:3])}...)")
    
    # 5. V√©rifier les colonnes cat√©gorielles avec trop de modalit√©s
    cat_cols = X.select_dtypes(include=['object', 'category']).columns
    high_cardinality_cols = []
    for col in cat_cols:
        n_unique = X[col].nunique()
        if n_unique > 100:
            high_cardinality_cols.append(f"{col} ({n_unique} valeurs)")
    
    if high_cardinality_cols:
        if len(high_cardinality_cols) <= 3:
            warnings.append(f"‚ö†Ô∏è Colonnes √† haute cardinalit√© : {', '.join(high_cardinality_cols)}")
        else:
            warnings.append(f"‚ö†Ô∏è {len(high_cardinality_cols)} colonnes √† haute cardinalit√© (peut ralentir l'entra√Ænement)")
    
    # 6. V√©rifier les valeurs infinies dans X
    num_cols = X.select_dtypes(include=['int64', 'float64']).columns
    inf_cols = []
    for col in num_cols:
        if ((X[col] == float('inf')) | (X[col] == float('-inf'))).any():
            inf_cols.append(col)
    
    if inf_cols:
        warnings.append(f"‚ö†Ô∏è Colonnes avec valeurs infinies : {', '.join(inf_cols[:5])}")
    
    # 7. V√©rifier la taille du dataset
    total_size_mb = (X.memory_usage(deep=True).sum() + y.memory_usage(deep=True)) / 1024 / 1024
    if total_size_mb > 500:
        warnings.append(f"‚ö†Ô∏è Dataset volumineux ({total_size_mb:.1f} MB) - l'entra√Ænement peut √™tre lent")
    
    # Afficher les r√©sultats
    if errors:
        for error in errors:
            st.error(error)
    
    if warnings:
        st.markdown("**‚ö†Ô∏è Avertissements de validation**")
        for warning in warnings:
            st.warning(warning)
        st.info("üí° Ces avertissements n'emp√™chent pas l'entra√Ænement, mais peuvent affecter les performances")
    
    if validation_passed and not errors:
        st.success(f"‚úÖ Validation r√©ussie : {X.shape[0]} lignes √ó {X.shape[1]} features")
    
    return validation_passed

def build_modeling_pipeline(model, X, do_scale=True, use_target_encoding=True):
    """
    Construit le pipeline de mod√©lisation avec gestion des variables cat√©gorielles
    
    Args:
        model: Mod√®le √† utiliser
        X: Donn√©es d'entra√Ænement
        do_scale: Si True, standardise les variables num√©riques
        use_target_encoding: Si True, utilise le Target Encoding pour les variables √† haute cardinalit√©
    """
    num_cols = X.select_dtypes(include="number").columns.tolist()
    cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
    
    # Convertir les cat√©gorielles en string
    if cat_cols:
        X[cat_cols] = X[cat_cols].astype(str)
    
    # Pipeline pour les colonnes num√©riques
    num_steps = []
    if num_cols:
        num_steps = [("imputer", SimpleImputer(strategy="median"))]
        if do_scale:
            num_steps.append(("scaler", StandardScaler()))
    
    # Gestion des variables cat√©gorielles
    transformers = []
    
    # 1. Colonnes num√©riques
    if num_cols:
        transformers.append(("num", Pipeline(num_steps), num_cols))
    
    # 2. Colonnes cat√©gorielles
    if cat_cols:
        # S√©paration basse/√©lev√©e cardinalit√©
        low_card_cols = [col for col in cat_cols if X[col].nunique() <= 100]
        high_card_cols = [col for col in cat_cols if X[col].nunique() > 100]
        
        # Pipeline pour basse cardinalit√© (OneHot)
        if low_card_cols:
            cat_steps_low = Pipeline([
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
            ])
            transformers.append(("cat_low", cat_steps_low, low_card_cols))
        
        # Pipeline pour haute cardinalit√© (Target Encoding)
        if high_card_cols and use_target_encoding:
            st.warning(f"‚ö†Ô∏è Colonnes √† haute cardinalit√© d√©tect√©es : {', '.join(high_card_cols)}")
            st.info("Utilisation de Target Encoding pour ces variables")
            
            cat_steps_high = Pipeline([
                ("imputer", SimpleImputer(strategy="most_frequent")),
                ("target_enc", TargetEncoder())
            ])
            transformers.append(("cat_high", cat_steps_high, high_card_cols))
        elif high_card_cols:
            st.warning(f"‚ö†Ô∏è Colonnes √† haute cardinalit√© ignor√©es : {', '.join(high_card_cols)}")
    
    # Cr√©ation du ColumnTransformer
    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder="drop",
        verbose_feature_names_out=False
    )
    
    # Cr√©ation du pipeline final
    pipeline = Pipeline([
        ("preprocessor", preprocessor),
        ("model", model)
    ])
    
    return pipeline

def run_modeling(df: pd.DataFrame) -> dict:
    st.subheader("‚ö° Mod√©lisation interactive")
    
    # D√©tecter si on vient de la comparaison
    from_comparison = "best_model_name" in st.session_state and "comparison_results" in st.session_state
    
    if from_comparison:
        best_model = st.session_state['best_model_name']
        comparison_results = st.session_state['comparison_results']
        
        # R√©cup√©rer le score du meilleur mod√®le
        best_row = comparison_results[comparison_results['Mod√®le'] == best_model]
        if not best_row.empty:
            # Trouver la colonne de score principal (v√©rifier diff√©rentes variantes)
            score_col = None
            score_value = None
            
            # Essayer diff√©rentes variantes de noms de colonnes
            possible_score_cols = ['ACCURACY', 'Accuracy', 'accuracy', 'R2', 'R¬≤', 'r2']
            for col in possible_score_cols:
                if col in comparison_results.columns:
                    score_col = col
                    score_value = best_row[col].values[0]
                    break
            
            if score_col and score_value is not None:
                st.success(f"üèÜ **Meilleur mod√®le de la comparaison** : {best_model} (Score: {score_value:.4f})")
                st.info("üí° Les hyperparam√®tres du meilleur mod√®le sont pr√©-remplis. Vous pouvez les modifier pour optimiser davantage.")
            else:
                st.success(f"üèÜ **Meilleur mod√®le de la comparaison** : {best_model}")
                st.info("üí° Les hyperparam√®tres du meilleur mod√®le sont pr√©-remplis. Vous pouvez les modifier pour optimiser davantage.")
        else:
            st.success(f"üèÜ **Meilleur mod√®le d√©tect√©** : {best_model}")
            st.info("üí° Vous pouvez affiner ce mod√®le ou en choisir un autre")

    cols = df.columns.tolist()
    
    # Pr√©-remplir la cible si elle existe d√©j√†
    default_target = ""
    if "y_train" in st.session_state and hasattr(st.session_state["y_train"], "name"):
        default_target = st.session_state["y_train"].name
    
    target_index = 0
    if default_target and default_target in cols:
        target_index = cols.index(default_target) + 1
    
    target = st.selectbox("Choisir la variable cible", [""] + cols, index=target_index)
    if not target:
        st.info("S√©lectionne une variable cible pour lancer l'entra√Ænement.")
        st.stop()

    X = df.drop(columns=[target])
    y = df[target]
    
    # Validation compl√®te des donn√©es (silencieuse si pas d'erreurs)
    if not _validate_data_for_modeling(X, y):
        st.error("‚ùå Les donn√©es ne sont pas valides pour la mod√©lisation")
        st.info("üí° Corrigez les erreurs ci-dessus avant de continuer")
        st.stop()
    
    # V√©rifier les valeurs manquantes dans y
    y_missing = y.isna().sum()
    if y_missing > 0:
        st.warning(f"‚ö†Ô∏è Variable cible contient {y_missing} valeurs manquantes ({y_missing/len(y)*100:.1f}%)")
        
        action = st.radio(
            "Comment traiter les valeurs manquantes dans la cible ?",
            ["Supprimer les lignes", "Imputer (moyenne/mode)", "Annuler"],
            key="missing_target_action"
        )
        
        if action == "Annuler":
            st.info("Veuillez nettoyer vos donn√©es avant la mod√©lisation")
            st.stop()
        elif action == "Supprimer les lignes":
            valid_idx = y.notna()
            X = X[valid_idx].reset_index(drop=True)
            y = y[valid_idx].reset_index(drop=True)
            st.success(f"‚úÖ {y_missing} lignes supprim√©es. Nouvelles dimensions : {len(y)} lignes")
        else:  # Imputer
            if y.dtype in ['object', 'category']:
                mode_val = y.mode()[0] if not y.mode().empty else y.dropna().iloc[0]
                y = y.fillna(mode_val)
                st.success(f"‚úÖ Valeurs manquantes imput√©es avec le mode : {mode_val}")
            else:
                mean_val = y.mean()
                y = y.fillna(mean_val)
                st.success(f"‚úÖ Valeurs manquantes imput√©es avec la moyenne : {mean_val:.2f}")
    
    # V√©rifier les valeurs infinies dans y (pour r√©gression)
    if y.dtype in ['int64', 'float64']:
        y_inf = (~y.isna() & ((y == float('inf')) | (y == float('-inf')))).sum()
        if y_inf > 0:
            st.warning(f"‚ö†Ô∏è Variable cible contient {y_inf} valeurs infinies")
            y = y.replace([float('inf'), float('-inf')], pd.NA)
            y = y.fillna(y.median())
            st.success(f"‚úÖ Valeurs infinies remplac√©es par la m√©diane")
    
    # S√©lection du type de t√¢che avec UI (d√©tection auto + choix utilisateur)
    task = model_utils.select_task_type_with_ui(y, key_suffix="modeling")
    
    # Afficher les infos de mani√®re compacte
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üìè Lignes", f"{len(y):,}")
    with col2:
        st.metric("üéØ Uniques", y.nunique())
    with col3:
        if y.dtype in ['int64', 'float64']:
            st.metric("üìà Moyenne", f"{y.mean():.2f}")
        else:
            st.metric("üìå Mode", str(y.mode()[0])[:10] if not y.mode().empty else "N/A")
    
    st.markdown("---")

    # Option pour activer/d√©sactiver le Target Encoding
    use_target_encoding = st.checkbox(
        "Utiliser Target Encoding pour les variables √† haute cardinalit√©",
        value=True,
        help="Active le Target Encoding pour les variables cat√©gorielles avec plus de 100 valeurs uniques"
    )
    
    # Configuration compacte
    st.markdown("### ‚öôÔ∏è Configuration")
    col1, col2, col3 = st.columns(3)
    with col1:
        test_size = st.slider("Taille test (%)", 5, 50, 20) / 100.0
    with col2:
        random_state = int(st.number_input("Seed", value=42))
    with col3:
        do_scale = st.checkbox("Standardiser", value=True)
    
    # D√©finir tous les mod√®les disponibles
    st.markdown("### üéØ S√©lection du Mod√®le")
    
    # Mapper les noms de la comparaison vers les choix de modeling
    model_mapping = {
        "Random Forest": "random_forest",
        "Gradient Boosting": "gradient_boosting",
        "Logistic Regression": "logistic_regression",
        "Linear Regression": "linear_regression",
        "Ridge": "ridge",
        "Lasso": "lasso",
        "AdaBoost": "adaboost",
        "Extra Trees": "extra_trees",
        "Decision Tree": "decision_tree",
        "K-Nearest Neighbors": "knn",
        "SVM": "svm",
        "SVR": "svr"
    }
    
    # Liste compl√®te des mod√®les disponibles
    if task == "classification":
        available_models = [
            "Random Forest", "Gradient Boosting", "Logistic Regression",
            "AdaBoost", "Extra Trees", "Decision Tree", "K-Nearest Neighbors", "SVM"
        ]
    else:
        available_models = [
            "Random Forest", "Gradient Boosting", "Linear Regression",
            "Ridge", "Lasso", "AdaBoost", "Extra Trees", "Decision Tree",
            "K-Nearest Neighbors", "SVR"
        ]
    
    # Si on vient de la comparaison, proposer les mod√®les test√©s
    if from_comparison and "comparison_results" in st.session_state:
        comparison_models = st.session_state["comparison_results"]["Mod√®le"].tolist()
        best_model_name = st.session_state.get("best_model_name", comparison_models[0])
        
        # Filtrer les mod√®les disponibles pour ne garder que ceux de la comparaison
        models_to_show = [m for m in comparison_models if m in available_models]
        
        # S√©lection avec le meilleur mod√®le par d√©faut
        model_display_choice = st.selectbox(
            "Choisir le mod√®le √† affiner",
            models_to_show,
            index=models_to_show.index(best_model_name) if best_model_name in models_to_show else 0,
            help="Le meilleur mod√®le de la comparaison est s√©lectionn√© par d√©faut"
        )
    else:
        # S√©lection parmi tous les mod√®les disponibles
        model_display_choice = st.selectbox(
            "Choisir un mod√®le",
            available_models,
            help="S√©lectionnez le mod√®le √† entra√Æner"
        )
    
    # Convertir vers le format interne
    model_choice = model_mapping.get(model_display_choice, "random_forest")
    
    st.info(f"üí° Mod√®le s√©lectionn√© : **{model_display_choice}**")
    
    # Hyperparam√®tres par d√©faut
    default_params = {
        'rf_n_estimators': 100, 'rf_max_depth': 0,
        'gb_n_estimators': 100, 'gb_max_depth': 3, 'gb_lr': 0.1,
        'ab_n_estimators': 50, 'ab_lr': 1.0,
        'et_n_estimators': 100, 'et_max_depth': 0,
        'dt_max_depth': 0, 'dt_min_samples_split': 2,
        'knn_n_neighbors': 5,
        'svm_C': 1.0, 'svm_kernel': 'rbf',
        'ridge_alpha': 1.0,
        'lasso_alpha': 1.0
    }
    
    # Extraire les hyperparam√®tres du meilleur mod√®le de la comparaison si disponible
    if from_comparison and "best_model" in st.session_state and model_display_choice == st.session_state.get("best_model_name"):
        try:
            best_pipeline = st.session_state["best_model"]
            if best_pipeline and hasattr(best_pipeline, "named_steps"):
                best_model_obj = best_pipeline.named_steps.get("model")
                
                if best_model_obj:
                    # Extraire les param√®tres selon le type de mod√®le
                    params = best_model_obj.get_params()
                    
                    # Random Forest
                    if "RandomForest" in str(type(best_model_obj)):
                        default_params['rf_n_estimators'] = params.get('n_estimators', 100)
                        default_params['rf_max_depth'] = params.get('max_depth') or 0
                    
                    # Gradient Boosting
                    elif "GradientBoosting" in str(type(best_model_obj)):
                        default_params['gb_n_estimators'] = params.get('n_estimators', 100)
                        default_params['gb_max_depth'] = params.get('max_depth', 3)
                        default_params['gb_lr'] = params.get('learning_rate', 0.1)
                    
                    # AdaBoost
                    elif "AdaBoost" in str(type(best_model_obj)):
                        default_params['ab_n_estimators'] = params.get('n_estimators', 50)
                        default_params['ab_lr'] = params.get('learning_rate', 1.0)
                    
                    # Extra Trees
                    elif "ExtraTrees" in str(type(best_model_obj)):
                        default_params['et_n_estimators'] = params.get('n_estimators', 100)
                        default_params['et_max_depth'] = params.get('max_depth') or 0
                    
                    # Decision Tree
                    elif "DecisionTree" in str(type(best_model_obj)):
                        default_params['dt_max_depth'] = params.get('max_depth') or 0
                        default_params['dt_min_samples_split'] = params.get('min_samples_split', 2)
                    
                    # KNN
                    elif "KNeighbors" in str(type(best_model_obj)):
                        default_params['knn_n_neighbors'] = params.get('n_neighbors', 5)
                    
                    # SVM/SVR
                    elif "SVC" in str(type(best_model_obj)) or "SVR" in str(type(best_model_obj)):
                        default_params['svm_C'] = params.get('C', 1.0)
                        default_params['svm_kernel'] = params.get('kernel', 'rbf')
                    
                    # Ridge
                    elif "Ridge" in str(type(best_model_obj)):
                        default_params['ridge_alpha'] = params.get('alpha', 1.0)
                    
                    # Lasso
                    elif "Lasso" in str(type(best_model_obj)):
                        default_params['lasso_alpha'] = params.get('alpha', 1.0)
                    
                    st.success("‚ú® Hyperparam√®tres du meilleur mod√®le charg√©s automatiquement")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Impossible d'extraire les hyperparam√®tres : {str(e)}")
    
    st.markdown("### ‚öôÔ∏è Configuration des Hyperparam√®tres")
    
    # Afficher uniquement les hyperparam√®tres du mod√®le s√©lectionn√©
    if model_choice == "random_forest":
        st.markdown("**Random Forest**")
        rf_n_estimators = int(st.number_input("Nombre d'arbres (n_estimators)", 10, 1000, default_params['rf_n_estimators'], key="rf_n_est"))
        rf_max_depth = int(st.number_input("Profondeur max (0 = illimit√©e)", 0, 50, default_params['rf_max_depth'], key="rf_depth"))
    
    elif model_choice == "gradient_boosting":
        st.markdown("**Gradient Boosting**")
        gb_n_estimators = int(st.number_input("Nombre d'arbres (n_estimators)", 10, 1000, default_params['gb_n_estimators'], key="gb_n_est"))
        gb_max_depth = int(st.number_input("Profondeur max", 1, 20, default_params['gb_max_depth'], key="gb_depth"))
        gb_lr = float(st.number_input("Taux d'apprentissage (learning_rate)", 0.01, 1.0, default_params['gb_lr'], key="gb_lr"))
    
    elif model_choice == "adaboost":
        st.markdown("**AdaBoost**")
        ab_n_estimators = int(st.number_input("Nombre d'estimateurs", 10, 500, default_params['ab_n_estimators'], key="ab_n_est"))
        ab_lr = float(st.number_input("Taux d'apprentissage", 0.01, 2.0, default_params['ab_lr'], key="ab_lr"))
    
    elif model_choice == "extra_trees":
        st.markdown("**Extra Trees**")
        et_n_estimators = int(st.number_input("Nombre d'arbres", 10, 1000, default_params['et_n_estimators'], key="et_n_est"))
        et_max_depth = int(st.number_input("Profondeur max (0 = illimit√©e)", 0, 50, default_params['et_max_depth'], key="et_depth"))
    
    elif model_choice == "decision_tree":
        st.markdown("**Decision Tree**")
        dt_max_depth = int(st.number_input("Profondeur max (0 = illimit√©e)", 0, 50, default_params['dt_max_depth'], key="dt_depth"))
        dt_min_samples_split = int(st.number_input("Min samples split", 2, 20, default_params['dt_min_samples_split'], key="dt_split"))
    
    elif model_choice == "knn":
        st.markdown("**K-Nearest Neighbors**")
        knn_n_neighbors = int(st.number_input("Nombre de voisins (k)", 1, 50, default_params['knn_n_neighbors'], key="knn_k"))
    
    elif model_choice in ["svm", "svr"]:
        st.markdown("**Support Vector Machine**")
        svm_C = float(st.number_input("Param√®tre C", 0.01, 100.0, default_params['svm_C'], key="svm_c"))
        svm_kernel = st.selectbox("Kernel", ["rbf", "linear", "poly"], index=0, key="svm_kernel")
    
    elif model_choice == "ridge":
        st.markdown("**Ridge Regression**")
        ridge_alpha = float(st.number_input("Alpha (r√©gularisation)", 0.01, 100.0, default_params['ridge_alpha'], key="ridge_alpha"))
    
    elif model_choice == "lasso":
        st.markdown("**Lasso Regression**")
        lasso_alpha = float(st.number_input("Alpha (r√©gularisation)", 0.01, 100.0, default_params['lasso_alpha'], key="lasso_alpha"))
    
    elif model_choice in ["logistic_regression", "linear_regression"]:
        st.markdown(f"**{model_display_choice}**")
        st.info("Ce mod√®le n'a pas d'hyperparam√®tres √† configurer.")

    if st.button("üöÄ Lancer l'entra√Ænement"):
        with st.spinner("Pr√©paration des donn√©es..."):
            # Pr√©paration des donn√©es
            num_cols = X.select_dtypes(include="number").columns.tolist()
            cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()
        
            if cat_cols:
                X[cat_cols] = X[cat_cols].astype(str)
        
            # Utiliser la fonction build_modeling_pipeline
            pipeline = build_modeling_pipeline(
                model=None,  # Le mod√®le sera ajout√© plus tard
                X=X,
                do_scale=do_scale,
                use_target_encoding=use_target_encoding
            )
        
        
        # Choix du mod√®le avec tous les hyperparam√®tres
        if model_choice == "random_forest":
            if task == "classification":
                model = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=None if rf_max_depth==0 else rf_max_depth, random_state=random_state)
            else:
                model = RandomForestRegressor(n_estimators=rf_n_estimators, max_depth=None if rf_max_depth==0 else rf_max_depth, random_state=random_state)
        
        elif model_choice == "gradient_boosting":
            if task == "classification":
                model = GradientBoostingClassifier(n_estimators=gb_n_estimators, max_depth=gb_max_depth, learning_rate=gb_lr, random_state=random_state)
            else:
                model = GradientBoostingRegressor(n_estimators=gb_n_estimators, max_depth=gb_max_depth, learning_rate=gb_lr, random_state=random_state)
        
        elif model_choice == "adaboost":
            if task == "classification":
                model = AdaBoostClassifier(n_estimators=ab_n_estimators, learning_rate=ab_lr, random_state=random_state)
            else:
                model = AdaBoostRegressor(n_estimators=ab_n_estimators, learning_rate=ab_lr, random_state=random_state)
        
        elif model_choice == "extra_trees":
            if task == "classification":
                model = ExtraTreesClassifier(n_estimators=et_n_estimators, max_depth=None if et_max_depth==0 else et_max_depth, random_state=random_state)
            else:
                model = ExtraTreesRegressor(n_estimators=et_n_estimators, max_depth=None if et_max_depth==0 else et_max_depth, random_state=random_state)
        
        elif model_choice == "decision_tree":
            if task == "classification":
                model = DecisionTreeClassifier(max_depth=None if dt_max_depth==0 else dt_max_depth, min_samples_split=dt_min_samples_split, random_state=random_state)
            else:
                model = DecisionTreeRegressor(max_depth=None if dt_max_depth==0 else dt_max_depth, min_samples_split=dt_min_samples_split, random_state=random_state)
        
        elif model_choice == "knn":
            if task == "classification":
                model = KNeighborsClassifier(n_neighbors=knn_n_neighbors)
            else:
                model = KNeighborsRegressor(n_neighbors=knn_n_neighbors)
        
        elif model_choice == "svm":
            model = SVC(C=svm_C, kernel=svm_kernel, random_state=random_state)
        
        elif model_choice == "svr":
            model = SVR(C=svm_C, kernel=svm_kernel)
        
        elif model_choice == "ridge":
            model = Ridge(alpha=ridge_alpha, random_state=random_state)
        
        elif model_choice == "lasso":
            model = Lasso(alpha=lasso_alpha, random_state=random_state)
        
        elif model_choice == "logistic_regression":
            model = LogisticRegression(max_iter=1000, random_state=random_state)
        
        elif model_choice == "linear_regression":
            model = LinearRegression()
        
        else:
            # Fallback
            if task == "classification":
                model = RandomForestClassifier(random_state=random_state)
            else:
                model = RandomForestRegressor(random_state=random_state)

        preprocessor = pipeline.named_steps['preprocessor']

        pipe = Pipeline([
            ('preprocessor', preprocessor),
            ('model', model)  # Le mod√®le s√©lectionn√©
        ])
        
        # Split & train avec gestion d'erreurs robuste
        try:
            X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=test_size, random_state=random_state)
            
            with st.spinner("üîÑ Entra√Ænement du mod√®le en cours..."):
                pipe.fit(X_train, y_train)
                preds = pipe.predict(X_test)
                preds_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(pipe, "predict_proba") else None
                
        except ValueError as e:
            st.error(f"‚ùå **Erreur de donn√©es** : {str(e)}")
            st.info("üí° **Suggestions** :")
            st.markdown("""
            - V√©rifiez que vos donn√©es sont compatibles avec le mod√®le s√©lectionn√©
            - Assurez-vous qu'il n'y a pas de valeurs infinies ou NaN dans les features
            - Essayez de r√©duire le nombre de colonnes cat√©gorielles avec trop de modalit√©s
            """)
            st.stop()
            
        except MemoryError:
            st.error("‚ùå **M√©moire insuffisante** pour entra√Æner ce mod√®le")
            st.info("üí° **Suggestions** :")
            st.markdown("""
            - R√©duisez la taille de votre dataset (√©chantillonnage)
            - Choisissez un mod√®le plus simple (ex: Logistic Regression au lieu de Random Forest)
            - R√©duisez le nombre de features
            """) # Ajout de la parenth√®se fermante ici
            st.stop()
            
        except Exception as e:
            st.error(f"‚ùå **Erreur inattendue lors de l'entra√Ænement** : {str(e)}")
            st.markdown("---")
            st.markdown("**üêõ D√©tails techniques :**")
            st.exception(e)
            st.markdown("---")
            st.info("üí° Essayez de recharger vos donn√©es ou de choisir un autre mod√®le")
            st.stop()

        # √âvaluation (metrics utilitaires)
        if task == "classification":
            metrics_result = metrics.classification_metrics(y_test, preds)
        else:
            metrics_result = metrics.regression_metrics(y_test, preds)

        metrics_display = _format_metrics(metrics_result, decimals=4)
        st.write("üìä **Metrics (test)** :")
        st.json(metrics_display)

        # Sauvegarde mod√®le et datasets avec gestion d'erreurs
        # Nettoyer le nom de la cible pour √©viter les caract√®res sp√©ciaux
        safe_target = target.replace("/", "_").replace("\\", "_").replace(" ", "_").replace(":", "_")
        
        # Sauvegarde du mod√®le
        try:
            helpers.ensure_dir("outputs/models")
            model_path = f"outputs/models/model_{safe_target}.pkl"
            joblib.dump(pipe, model_path)
            st.success(f"‚úÖ Mod√®le sauvegard√© : {model_path}")
        except PermissionError:
            st.warning("‚ö†Ô∏è Impossible de sauvegarder le mod√®le : permissions insuffisantes")
            st.info("üí° Le mod√®le reste disponible dans la session en cours")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Impossible de sauvegarder le mod√®le : {str(e)}")
            st.info("üí° Le mod√®le reste disponible dans la session en cours")
        
        # Sauvegarde des datasets
        try:
            helpers.ensure_dir("outputs/data")
            X_train.assign(**{target: y_train}).to_csv(f"outputs/data/train_{safe_target}.csv", index=False)
            X_test.assign(**{target: y_test}).to_csv(f"outputs/data/test_{safe_target}.csv", index=False)
            st.success(f"‚úÖ Datasets sauvegard√©s dans outputs/data/")
        except PermissionError:
            st.warning("‚ö†Ô∏è Impossible de sauvegarder les datasets : permissions insuffisantes")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Impossible de sauvegarder les datasets : {str(e)}")
        
        st.success("‚úÖ Mod√®le entra√Æn√© avec succ√®s !")

        # Stocker dans session_state pour reporting/evaluation
        st.session_state.update({
            "model": pipe,
            "X_train": X_train,
            "X_test": X_test,
            "y_train": y_train,
            "y_test": y_test,
            "task": task,
            "task_type": task,
            "y_pred": preds,
            "y_pred_proba": preds_proba,
            "evaluation_metrics": pd.DataFrame([metrics_display]),
            "current_model_name": model_display_choice,  # Stocker le nom du mod√®le pour l'√©valuation
            "use_target_encoding": use_target_encoding
        })

        # Feature importance si disponible (essayer d'extraire proprement)
        try:
            m = pipe.named_steps["model"]
            if hasattr(m, "feature_importances_") and len(num_cols) > 0:
                # get feature names from preprocessor
                try:
                    feature_names = pipe.named_steps["preprocessor"].get_feature_names_out()
                except:
                    feature_names = num_cols + cat_cols
                fi = pd.Series(m.feature_importances_, index=feature_names).sort_values(ascending=False)
                st.session_state["feature_importance"] = fi
        except Exception:
            pass

        return {
            "pipeline": pipe,
            "X_train": X_train,
            "X_test": X_test,
            "y_train": y_train,
            "y_test": y_test,
            "task": task,
            "y_pred_proba": preds_proba
        }

    st.stop()